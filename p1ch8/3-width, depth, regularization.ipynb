{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the CNN's capacity, regularization and more\n",
    "===============\n",
    "\n",
    "Starts at section 8.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x107d87d30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.set_printoptions(edgeitems=2)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ASCII plots of training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asciichartpy\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used for logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For loading previously computed model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selects device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selected appropriate device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available(): \n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane','automobile','bird','cat','deer',\n",
    "               'dog','frog','horse','ship','truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "data_path = '../data-unversioned/p1ch6/'\n",
    "cifar10 = datasets.CIFAR10(\n",
    "    data_path, train=True, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar10_val = datasets.CIFAR10(\n",
    "    data_path, train=False, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: 0, 2: 1}\n",
    "class_names = ['airplane', 'bird']\n",
    "cifar2 = [(img, label_map[label])\n",
    "          for img, label in cifar10\n",
    "          if label in [0, 2]]\n",
    "cifar2_val = [(img, label_map[label])\n",
    "              for img, label in cifar10_val\n",
    "              if label in [0, 2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful definitions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
    "                                           shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64,\n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop, including progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    lossList=[]\n",
    "    epochList=[]\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=device)  # moves data to GPU\n",
    "            labels = labels.to(device=device) # moves labels to GPU\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        lossList.append(np.log10(loss_train/len(train_loader)))\n",
    "        epochList.append(epoch)\n",
    "        clear_output(wait=True)\n",
    "        print(\"              \",datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\"| Epoch\", epoch, \"| Loss\",round(loss_train/len(train_loader),4))\n",
    "        print(asciichartpy.plot(lossList, {'height': 10}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gets accuracy from training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, train_loader, val_loader):\n",
    "    accdict = {}\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in loader:\n",
    "                imgs = imgs.to(device=device)\n",
    "                labels = labels.to(device=device)\n",
    "                outputs = model(imgs)\n",
    "                _, predicted = torch.max(outputs, dim=1) # <1>\n",
    "                total += labels.shape[0]\n",
    "                correct += int((predicted == labels).sum())\n",
    "\n",
    "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))\n",
    "        accdict[name] = correct / total\n",
    "    return accdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increasing the width\n",
    "\n",
    "Let's increase the network's width and see the effect on performance and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetWidth(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 16, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(16 * 8 * 8, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 16 * 8 * 8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trains the wider CNN on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               2024-10-30 10:16:54 | Epoch 100 | Loss 0.0845\n",
      "   -0.26  ┼╮\n",
      "   -0.33  ┤╰╮\n",
      "   -0.41  ┤ ╰─╮\n",
      "   -0.48  ┤   ╰──────────╮\n",
      "   -0.55  ┤              ╰──────────────╮\n",
      "   -0.63  ┤                             ╰──────────────╮\n",
      "   -0.70  ┤                                            ╰─────────────╮\n",
      "   -0.78  ┤                                                          ╰───────────╮\n",
      "   -0.85  ┤                                                                      ╰───────────╮\n",
      "   -0.92  ┤                                                                                  ╰─────────╮\n",
      "   -1.00  ┤                                                                                            ╰──────\n",
      "   -1.07  ┤\n",
      "CPU times: user 52.8 s, sys: 8.16 s, total: 1min\n",
      "Wall time: 1min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = NetWidth().to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.97\n",
      "Accuracy val: 0.89\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': 0.9653, 'val': 0.889}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loads accuracy from previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dictionary from the binary file\n",
    "with open('model-accuracy-dict.pkl', 'rb') as pickleFile:\n",
    "    all_acc_dict = pickle.load(pickleFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.97\n",
      "Accuracy val: 0.89\n"
     ]
    }
   ],
   "source": [
    "all_acc_dict[\"width\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More elegance\n",
    "\n",
    "One could define a class that takes as input the width of the CNN. Here is how that would go:\n",
    "\n",
    "```python\n",
    "class NetWidth(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3,\n",
    "                               padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "```\n",
    "\n",
    "The training would proceed in essentially the same way: \n",
    "\n",
    "```python\n",
    "model = NetWidth(n_chans1=32).to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting the number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38386"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 regularization\n",
    "\n",
    "Implementing weight penalties. We can implement regularization pretty easily by adding a term to the loss. After computing the loss, whatever the loss function is, we can iterate the parameters of the model, sum their respective square (for L2) or abs (for L1), and backpropagate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop_l2reg(n_epochs, optimizer, model, loss_fn,\n",
    "                        train_loader):\n",
    "    lossList=[]\n",
    "    epochList=[]\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            l2_lambda = 0.001\n",
    "            l2_norm = sum(p.pow(2.0).sum()\n",
    "                          for p in model.parameters())  # L2 regularization\n",
    "            loss = loss + l2_lambda * l2_norm\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_train += loss.item()\n",
    "\n",
    "        lossList.append(np.log10(loss_train/len(train_loader)))\n",
    "        epochList.append(epoch)\n",
    "        clear_output(wait=True)\n",
    "        print(\"              \",datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\"| Epoch\", epoch, \"| Loss\",round(loss_train/len(train_loader),4))\n",
    "        print(asciichartpy.plot(lossList, {'height': 10}))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loads trained model from disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN architecture, as defined in the previous notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8 * 8 * 8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/15/7jdf1bc547qf4cwtcvv8g05h0000gn/T/ipykernel_27537/517758729.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_model.load_state_dict(torch.load(data_path\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = Net().to(device=device)\n",
    "loaded_model.load_state_dict(torch.load(data_path\n",
    "                                        + 'birds_vs_airplanes.pt',\n",
    "                                        map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               2024-10-30 10:36:59 | Epoch 100 | Loss 0.2055\n",
      "   -0.22  ┤\n",
      "   -0.27  ┼╮\n",
      "   -0.31  ┤╰╮\n",
      "   -0.35  ┤ ╰╮\n",
      "   -0.39  ┤  ╰─╮\n",
      "   -0.43  ┤    ╰───╮\n",
      "   -0.48  ┤        ╰──────────╮\n",
      "   -0.52  ┤                   ╰─────────────╮\n",
      "   -0.56  ┤                                 ╰───────────────╮\n",
      "   -0.60  ┤                                                 ╰───────────────────╮\n",
      "   -0.65  ┤                                                                     ╰─────────────────────╮\n",
      "   -0.69  ┤                                                                                           ╰───────\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Net().to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop_l2reg(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inserts accuracy into dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.91\n",
      "Accuracy val: 0.89\n"
     ]
    }
   ],
   "source": [
    "all_acc_dict[\"l2 reg\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The idea behind dropout is indeed simple: zero out a random fraction of outputs from neurons across the network, where the randomization happens at each training iteration. This procedure effectively generates slightly different models with different neuron topologies at each iteration, giving neurons in the model less chance to coordinate in the memorization process that happens during overfitting. An alternative point of view is that dropout perturbs the features being generated by the model, exerting an effect that is close to augmentation, but this time throughout the network.\n",
    "> \n",
    "> In PyTorch, we can implement dropout in a model by adding an nn.Dropout module between the nonlinear activation function and the linear or convolutional module of the subsequent layer. As an argument, we need to specify the probability with which inputs will be zeroed out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetDropout(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv1_dropout = nn.Dropout2d(p=0.4) # <=== DROPOUT\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3,\n",
    "                               padding=1)\n",
    "        self.conv2_dropout = nn.Dropout2d(p=0.4) # <=== DROPOUT\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = self.conv1_dropout(out) # <=== DROPOUT\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = self.conv2_dropout(out) # <=== DROPOUT\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               2024-10-30 10:43:40 | Epoch 100 | Loss 0.2406\n",
      "   -0.23  ┤\n",
      "   -0.27  ┼╮\n",
      "   -0.30  ┤╰╮\n",
      "   -0.34  ┤ ╰╮\n",
      "   -0.37  ┤  ╰──╮\n",
      "   -0.41  ┤     ╰──╮\n",
      "   -0.44  ┤        ╰───────╮\n",
      "   -0.48  ┤                ╰───────────────╮\n",
      "   -0.51  ┤                                ╰────────────────╮\n",
      "   -0.55  ┤                                                 ╰────────────────────╮\n",
      "   -0.58  ┤                                                                      ╰──────────────────────╮\n",
      "   -0.62  ┤                                                                                             ╰─────\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = NetDropout(n_chans1=32).to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.90\n",
      "Accuracy val: 0.88\n"
     ]
    }
   ],
   "source": [
    "all_acc_dict[\"dropout\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetBatchNorm(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv1_batchnorm = nn.BatchNorm2d(num_features=n_chans1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, \n",
    "                               padding=1)\n",
    "        self.conv2_batchnorm = nn.BatchNorm2d(num_features=n_chans1 // 2)\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv1_batchnorm(self.conv1(x))\n",
    "        out = F.max_pool2d(torch.tanh(out), 2)\n",
    "        out = self.conv2_batchnorm(self.conv2(out))\n",
    "        out = F.max_pool2d(torch.tanh(out), 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               2024-10-30 11:26:23 | Epoch 100 | Loss 0.0076\n",
      "   -0.33  ┤\n",
      "   -0.50  ┼─╮\n",
      "   -0.66  ┤ ╰───────────╮\n",
      "   -0.82  ┤             ╰──────────────╮\n",
      "   -0.98  ┤                            ╰───────────╮\n",
      "   -1.14  ┤                                        ╰─────────╮\n",
      "   -1.31  ┤                                                  ╰─────────╮╭╮   ╭──╮\n",
      "   -1.47  ┤                                                            ╰╯╰───╯  ╰╮\n",
      "   -1.63  ┤                                                                      ╰───────╮\n",
      "   -1.79  ┤                                                                              ╰──────╮\n",
      "   -1.96  ┤                                                                                     ╰─────────╮\n",
      "   -2.12  ┤                                                                                               ╰───\n",
      "CPU times: user 1min 6s, sys: 9.05 s, total: 1min 15s\n",
      "Wall time: 1min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = NetBatchNorm(n_chans1=32).to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.99\n",
      "Accuracy val: 0.88\n"
     ]
    }
   ],
   "source": [
    "all_acc_dict[\"batch_norm\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increasing the depth with ResNet\n",
    "\n",
    "Depth allows a model to better deal with hierarchical information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dealing with vanishing gradients with residual networks (ResNets). This opened the doors to networks with 100 layers, beating competitions in computer vision.\n",
    "\n",
    "The skip connection is the trick.\n",
    "\n",
    "![img description](IMG_EE367CCDE83A-1.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetDepth(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3,\n",
    "                               padding=1)\n",
    "        self.conv3 = nn.Conv2d(n_chans1 // 2, n_chans1 // 2,\n",
    "                               kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(4 * 4 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv3(out)), 2)\n",
    "        out = out.view(-1, 4 * 4 * self.n_chans1 // 2)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               2024-10-30 11:02:58 | Epoch 100 | Loss 0.098\n",
      "   -0.16  ┤\n",
      "   -0.24  ┼─╮\n",
      "   -0.32  ┤ ╰╮\n",
      "   -0.39  ┤  ╰─╮\n",
      "   -0.47  ┤    ╰───╮\n",
      "   -0.55  ┤        ╰─────────────────╮\n",
      "   -0.62  ┤                          ╰────────────────╮\n",
      "   -0.70  ┤                                           ╰───────────────╮\n",
      "   -0.78  ┤                                                           ╰────────────╮\n",
      "   -0.85  ┤                                                                        ╰───────────╮\n",
      "   -0.93  ┤                                                                                    ╰─────────╮\n",
      "   -1.01  ┤                                                                                              ╰────\n"
     ]
    }
   ],
   "source": [
    "model = NetDepth(n_chans1=32).to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.95\n",
      "Accuracy val: 0.89\n"
     ]
    }
   ],
   "source": [
    "all_acc_dict[\"depth\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet\n",
    "\n",
    "Effects of jump connections:\n",
    "\n",
    "> Thinking about backpropagation, we can appreciate that a skip connection, or a sequence of skip connections in a deep network, creates a direct path from the deeper parameters to the loss. This makes their contribution to the gradient of the loss more direct, as partial derivatives of the loss with respect to those parameters have a chance not to be multiplied by a long chain of other operations.\n",
    "> \n",
    "> It has been observed that skip connections have a beneficial effect on convergence especially in the initial phases of training. Also, the loss landscape of deep residual networks is a lot smoother than feed-forward networks of the same depth and width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetRes(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3,\n",
    "                               padding=1)\n",
    "        self.conv3 = nn.Conv2d(n_chans1 // 2, n_chans1 // 2,\n",
    "                               kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(4 * 4 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
    "        out1 = out\n",
    "        out = F.max_pool2d(torch.relu(self.conv3(out)) + out1, 2) # NOTICE THE SKIP CONNECTION\n",
    "        out = out.view(-1, 4 * 4 * self.n_chans1 // 2)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               2024-10-30 11:05:07 | Epoch 100 | Loss 0.1081\n",
      "   -0.18  ┼╮\n",
      "   -0.27  ┤╰╮\n",
      "   -0.35  ┤ ╰─╮\n",
      "   -0.43  ┤   ╰───────╮\n",
      "   -0.51  ┤           ╰──────────────╮\n",
      "   -0.60  ┤                          ╰───────────────╮\n",
      "   -0.68  ┤                                          ╰───────────────╮\n",
      "   -0.76  ┤                                                          ╰────────────╮\n",
      "   -0.84  ┤                                                                       ╰──────────╮\n",
      "   -0.93  ┤                                                                                  ╰─────────╮    ╭─\n",
      "   -1.01  ┤                                                                                            ╰────╯\n",
      "   -1.09  ┤\n"
     ]
    }
   ],
   "source": [
    "model = NetRes(n_chans1=32).to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.97\n",
      "Accuracy val: 0.90\n"
     ]
    }
   ],
   "source": [
    "all_acc_dict[\"res\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Very deep models\n",
    "\n",
    "How to build a NN with 100 layers?\n",
    "\n",
    "Define a building block (`ResBlock` below) and then build a network using a loop (`NetResDeep`).\n",
    "\n",
    "![img description](IMG_2E25026BA790-1.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create a module subclass whose sole job is to provide the computation for one block—that is, one group of convolutions, activation, and skip connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, n_chans):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(n_chans, n_chans, kernel_size=3,\n",
    "                              padding=1, bias=False)  # <1>\n",
    "        self.batch_norm = nn.BatchNorm2d(num_features=n_chans)\n",
    "        torch.nn.init.kaiming_normal_(self.conv.weight,\n",
    "                                      nonlinearity='relu')  # <2>\n",
    "        torch.nn.init.constant_(self.batch_norm.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.batch_norm.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.batch_norm(out)\n",
    "        out = torch.relu(out)\n",
    "        return out + x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the blocks, first in `init` we create `nn.Sequential` containing a list of `ResBlock` instances. `nn.Sequential` will ensure that the output of one block is used as input to the next. It will also ensure that all the parameters in the block are visible to Net. Then, in forward, we just call the sequential to traverse the 100 blocks and generate the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetResDeep(nn.Module):\n",
    "    def __init__(self, n_chans1=32, n_blocks=10):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.resblocks = nn.Sequential(\n",
    "            *(n_blocks * [ResBlock(n_chans=n_chans1)]))\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = self.resblocks(out)\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>ATTENTION</h4><p>\n",
    "This will take about an hour to train.\n",
    "</p></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               2024-10-30 12:01:53 | Epoch 100 | Loss 0.0488\n",
      "   -0.18  ┤\n",
      "   -0.29  ┼╮\n",
      "   -0.39  ┤╰╮\n",
      "   -0.50  ┤ ╰───╮\n",
      "   -0.60  ┤     ╰──────╮\n",
      "   -0.71  ┤            ╰─────╮     ╭╮  ╭╮                                 ╭╮\n",
      "   -0.81  ┤                  ╰─────╯╰──╯╰───╮╭╮ ╭╮                        │╰╮       ╭╮\n",
      "   -0.92  ┤                                 ╰╯╰─╯╰─╮ ╭╮  ╭╮           ╭╮  │ ╰─╮╭╮ ╭╮│╰────╮             ╭╮\n",
      "   -1.02  ┤                                        ╰─╯╰─╮│╰─╮         │╰╮ │   ╰╯╰─╯││     ╰───╮        ╭╯│\n",
      "   -1.13  ┤                                             ╰╯  ╰──╮╭─╮ ╭─╯ │ │        ││         ╰─╮╭─╮ ╭─╯ │\n",
      "   -1.23  ┤                                                    ╰╯ ╰─╯   ╰─╯        ╰╯           ╰╯ ╰╮│   ╰────\n",
      "   -1.34  ┤                                                                                         ╰╯\n",
      "CPU times: user 18min 16s, sys: 2min 2s, total: 20min 18s\n",
      "Wall time: 34min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = NetResDeep(n_chans1=32, n_blocks=100).to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=3e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.98\n",
      "Accuracy val: 0.88\n"
     ]
    }
   ],
   "source": [
    "all_acc_dict[\"res deep\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trn_acc = [v['train'] for k, v in all_acc_dict.items()]\n",
    "val_acc = [v['val'] for k, v in all_acc_dict.items()]\n",
    "\n",
    "width =0.3\n",
    "plt.bar(np.arange(len(trn_acc)), trn_acc, width=width, label='train')\n",
    "plt.bar(np.arange(len(val_acc))+ width, val_acc, width=width, label='val')\n",
    "plt.xticks(np.arange(len(val_acc))+ width/2, list(all_acc_dict.keys()),\n",
    "           rotation=60)\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim(0.7, 1)\n",
    "plt.savefig('accuracy_comparison.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

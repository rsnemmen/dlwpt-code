{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the CNN's capacity, regularization and more\n",
    "===============\n",
    "\n",
    "Starts at section 8.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x109f83d10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.set_printoptions(edgeitems=2)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ASCII plots of training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asciichartpy\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used for logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane','automobile','bird','cat','deer',\n",
    "               'dog','frog','horse','ship','truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "data_path = '../data-unversioned/p1ch6/'\n",
    "cifar10 = datasets.CIFAR10(\n",
    "    data_path, train=True, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar10_val = datasets.CIFAR10(\n",
    "    data_path, train=False, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: 0, 2: 1}\n",
    "class_names = ['airplane', 'bird']\n",
    "cifar2 = [(img, label_map[label])\n",
    "          for img, label in cifar10\n",
    "          if label in [0, 2]]\n",
    "cifar2_val = [(img, label_map[label])\n",
    "              for img, label in cifar10_val\n",
    "              if label in [0, 2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful definitions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improved code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    lossList=[]\n",
    "    epochList=[]\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):  # <2>\n",
    "        loss_train = 0.0\n",
    "        \n",
    "        for imgs, labels in train_loader:  # <3>\n",
    "            outputs = model(imgs)  # <4>\n",
    "            loss = loss_fn(outputs, labels)  # <5\n",
    "            optimizer.zero_grad()  # <6>\n",
    "            loss.backward()  # <7>\n",
    "            optimizer.step()  # <8>\n",
    "\n",
    "            loss_train += loss.item()  # <9>\n",
    "\n",
    "        lossList.append(np.log10(loss_train/len(train_loader)))\n",
    "        epochList.append(epoch)\n",
    "        clear_output(wait=True)\n",
    "        print(\"              \",datetime.datetime.now(),\"| Epoch\", epoch, \"| Loss\",round(loss_train/len(train_loader),4))\n",
    "        print(asciichartpy.plot(lossList, {'height': 10}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starts the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               2024-10-24 09:36:13.779163 | Epoch 100 | Loss 0.1488\n",
      "   -0.25  ┼╮\n",
      "   -0.30  ┤╰╮\n",
      "   -0.36  ┤ ╰─╮\n",
      "   -0.41  ┤   ╰─╮\n",
      "   -0.46  ┤     ╰─────╮\n",
      "   -0.51  ┤           ╰──────────────╮\n",
      "   -0.57  ┤                          ╰─────────────╮\n",
      "   -0.62  ┤                                        ╰────────────╮\n",
      "   -0.67  ┤                                                     ╰───────────────╮\n",
      "   -0.72  ┤                                                                     ╰──────────────╮╭╮\n",
      "   -0.78  ┤                                                                                    ╰╯╰────────────\n",
      "   -0.83  ┤\n",
      "CPU times: user 13min 5s, sys: 2min 25s, total: 15min 30s\n",
      "Wall time: 3min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
    "                                           shuffle=True)  # <1>\n",
    "\n",
    "model = Net()  #  <2>\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)  #  <3>\n",
    "loss_fn = nn.CrossEntropyLoss()  #  <4>\n",
    "\n",
    "training_loop(  # <5>\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
    "                                           shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64,\n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines method that computes accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, train_loader, val_loader):\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():  # <1>\n",
    "            for imgs, labels in loader:\n",
    "                outputs = model(imgs)\n",
    "                _, predicted = torch.max(outputs, dim=1) # <2>\n",
    "                total += labels.shape[0]  # <3>\n",
    "                correct += int((predicted == labels).sum())  # <4>\n",
    "\n",
    "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.43\n",
      "Accuracy val: 0.42\n"
     ]
    }
   ],
   "source": [
    "validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the fully connected model got only 79% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saves the model to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), data_path + 'birds_vs_airplanes.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When loading the model back, make sure you do not change anything in the class definition\n",
    "\n",
    "```python\n",
    "loaded_model = Net()  # <1>\n",
    "loaded_model.load_state_dict(torch.load(data_path\n",
    "                                        + 'birds_vs_airplanes.pt'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selected appropriate device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available(): \n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    lossList=[]\n",
    "    epochList=[]\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=device)  # moves data to GPU\n",
    "            labels = labels.to(device=device) # moves labels to GPU\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        lossList.append(np.log10(loss_train/len(train_loader)))\n",
    "        epochList.append(epoch)\n",
    "        clear_output(wait=True)\n",
    "        print(\"              \",datetime.datetime.now(),\"| Epoch\", epoch, \"| Loss\",round(loss_train/len(train_loader),4))\n",
    "        print(asciichartpy.plot(lossList, {'height': 10}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               2024-10-24 09:38:10.859721 | Epoch 100 | Loss 0.1492\n",
      "   -0.26  ┤\n",
      "   -0.32  ┼╮\n",
      "   -0.37  ┤╰─╮\n",
      "   -0.42  ┤  ╰─╮\n",
      "   -0.47  ┤    ╰──╮\n",
      "   -0.52  ┤       ╰──────────╮\n",
      "   -0.57  ┤                  ╰────────────╮\n",
      "   -0.62  ┤                               ╰────────────────╮\n",
      "   -0.67  ┤                                                ╰──────────────────╮\n",
      "   -0.72  ┤                                                                   ╰──────────────╮\n",
      "   -0.78  ┤                                                                                  ╰──────────────╮\n",
      "   -0.83  ┤                                                                                                 ╰─\n",
      "CPU times: user 39 s, sys: 6.09 s, total: 45.1 s\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
    "                                           shuffle=True)\n",
    "\n",
    "model = Net().to(device=device)  # moves model and parameters to GPU\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
    "                                           shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64,\n",
    "                                         shuffle=False)\n",
    "all_acc_dict = collections.OrderedDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (MPSFloatType) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m         accdict[name] \u001b[38;5;241m=\u001b[39m correct \u001b[38;5;241m/\u001b[39m total\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accdict\n\u001b[0;32m---> 20\u001b[0m all_acc_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbaseline\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[32], line 11\u001b[0m, in \u001b[0;36mvalidate\u001b[0;34m(model, train_loader, val_loader)\u001b[0m\n\u001b[1;32m      9\u001b[0m imgs \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     10\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 11\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# <1>\u001b[39;00m\n\u001b[1;32m     13\u001b[0m total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[24], line 12\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 12\u001b[0m     out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmax_pool2d(torch\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m), \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     13\u001b[0m     out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmax_pool2d(torch\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(out)), \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     14\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m8\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m8\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m8\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (MPSFloatType) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "def validate(model, train_loader, val_loader):\n",
    "    accdict = {}\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in loader:\n",
    "                imgs = imgs.to(device=device)\n",
    "                labels = labels.to(device=device)\n",
    "                outputs = model(imgs)\n",
    "                _, predicted = torch.max(outputs, dim=1) # <1>\n",
    "                total += labels.shape[0]\n",
    "                correct += int((predicted == labels).sum())\n",
    "\n",
    "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))\n",
    "        accdict[name] = correct / total\n",
    "    return accdict\n",
    "\n",
    "all_acc_dict[\"baseline\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = Net().to(device=device)\n",
    "loaded_model.load_state_dict(torch.load(data_path\n",
    "                                        + 'birds_vs_airplanes.pt',\n",
    "                                        map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increasing the network's width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's increase the network's width and see the effect on performance and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetWidth(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 16, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(16 * 8 * 8, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 16 * 8 * 8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trains the wider CNN on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               2024-10-24 18:02:25.625471 | Epoch 100 | Loss 0.0838\n",
      "   -0.26  ┼╮\n",
      "   -0.34  ┤╰─╮\n",
      "   -0.41  ┤  ╰─╮\n",
      "   -0.48  ┤    ╰──────────╮\n",
      "   -0.56  ┤               ╰─────────────╮\n",
      "   -0.63  ┤                             ╰─────────────╮\n",
      "   -0.71  ┤                                           ╰─────────────╮\n",
      "   -0.78  ┤                                                         ╰────────────╮\n",
      "   -0.85  ┤                                                                      ╰──────────╮\n",
      "   -0.93  ┤                                                                                 ╰──────────╮\n",
      "   -1.00  ┤                                                                                            ╰──────\n",
      "   -1.08  ┤\n",
      "CPU times: user 59.5 s, sys: 8.81 s, total: 1min 8s\n",
      "Wall time: 1min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = NetWidth().to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Mismatched Tensor types in NNPack convolutionOutput",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 8\u001b[0m, in \u001b[0;36mvalidate\u001b[0;34m(model, train_loader, val_loader)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# <1>\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m imgs, labels \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m----> 8\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m         _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# <2>\u001b[39;00m\n\u001b[1;32m     10\u001b[0m         total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# <3>\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[13], line 10\u001b[0m, in \u001b[0;36mNetWidth.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 10\u001b[0m     out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmax_pool2d(torch\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m), \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     11\u001b[0m     out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmax_pool2d(torch\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(out)), \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     12\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m16\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m8\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m8\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Mismatched Tensor types in NNPack convolutionOutput"
     ]
    }
   ],
   "source": [
    "validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More elegance\n",
    "\n",
    "One could define a class that takes as input the width of the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetWidth(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3,\n",
    "                               padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-16 23:12:22.646242 Epoch 1, Training loss 0.5444508081029176\n",
      "2020-01-16 23:12:26.841887 Epoch 10, Training loss 0.31707597633076323\n",
      "2020-01-16 23:12:31.310813 Epoch 20, Training loss 0.27454944429503886\n",
      "2020-01-16 23:12:35.891615 Epoch 30, Training loss 0.2425653456123012\n",
      "2020-01-16 23:12:40.420664 Epoch 40, Training loss 0.21338120942852298\n",
      "2020-01-16 23:12:44.954698 Epoch 50, Training loss 0.18698934290059813\n",
      "2020-01-16 23:12:49.527939 Epoch 60, Training loss 0.16319987830367816\n",
      "2020-01-16 23:12:54.380134 Epoch 70, Training loss 0.14089395708529054\n",
      "2020-01-16 23:12:59.247492 Epoch 80, Training loss 0.11998948957889703\n",
      "2020-01-16 23:13:04.068846 Epoch 90, Training loss 0.10076516851260783\n",
      "2020-01-16 23:13:08.913110 Epoch 100, Training loss 0.0832248356217032\n",
      "Accuracy train: 0.96\n",
      "Accuracy val: 0.90\n"
     ]
    }
   ],
   "source": [
    "model = NetWidth(n_chans1=32).to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "\n",
    "all_acc_dict[\"width\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting the number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38386"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop_l2reg(n_epochs, optimizer, model, loss_fn,\n",
    "                        train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            l2_lambda = 0.001\n",
    "            l2_norm = sum(p.pow(2.0).sum()\n",
    "                          for p in model.parameters())  # <1>\n",
    "            loss = loss + l2_lambda * l2_norm\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_train += loss.item()\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(\n",
    "                datetime.datetime.now(), epoch,\n",
    "                loss_train / len(train_loader)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-16 23:13:10.225792 Epoch 1, Training loss 0.584277889151482\n",
      "2020-01-16 23:13:17.795340 Epoch 10, Training loss 0.36633389723149073\n",
      "2020-01-16 23:13:26.277897 Epoch 20, Training loss 0.3225795095133933\n",
      "2020-01-16 23:13:35.341923 Epoch 30, Training loss 0.29615209541123383\n",
      "2020-01-16 23:13:44.351376 Epoch 40, Training loss 0.2775719240782367\n",
      "2020-01-16 23:13:53.296178 Epoch 50, Training loss 0.2636590329514947\n",
      "2020-01-16 23:14:02.220169 Epoch 60, Training loss 0.2515565001755763\n",
      "2020-01-16 23:14:11.076573 Epoch 70, Training loss 0.24007968713713299\n",
      "2020-01-16 23:14:20.807501 Epoch 80, Training loss 0.22931366546708307\n",
      "2020-01-16 23:14:31.504612 Epoch 90, Training loss 0.21898466424577556\n",
      "2020-01-16 23:14:41.934048 Epoch 100, Training loss 0.20924225397360552\n",
      "Accuracy train: 0.90\n",
      "Accuracy val: 0.87\n"
     ]
    }
   ],
   "source": [
    "model = Net().to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop_l2reg(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "all_acc_dict[\"l2 reg\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetDropout(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv1_dropout = nn.Dropout2d(p=0.4)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3,\n",
    "                               padding=1)\n",
    "        self.conv2_dropout = nn.Dropout2d(p=0.4)\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = self.conv1_dropout(out)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = self.conv2_dropout(out)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-16 23:14:42.863912 Epoch 1, Training loss 0.5800061457476039\n",
      "2020-01-16 23:14:47.868802 Epoch 10, Training loss 0.38074702850192976\n",
      "2020-01-16 23:14:53.145910 Epoch 20, Training loss 0.34908065987620385\n",
      "2020-01-16 23:14:58.056904 Epoch 30, Training loss 0.32977743029214773\n",
      "2020-01-16 23:15:03.131635 Epoch 40, Training loss 0.3125769621247699\n",
      "2020-01-16 23:15:08.321374 Epoch 50, Training loss 0.29207915010725616\n",
      "2020-01-16 23:15:13.535053 Epoch 60, Training loss 0.28212467301043737\n",
      "2020-01-16 23:15:18.876606 Epoch 70, Training loss 0.2723999054758412\n",
      "2020-01-16 23:15:24.114116 Epoch 80, Training loss 0.2627566327714616\n",
      "2020-01-16 23:15:29.342708 Epoch 90, Training loss 0.2537129214804643\n",
      "2020-01-16 23:15:34.594518 Epoch 100, Training loss 0.23995957129700168\n",
      "Accuracy train: 0.89\n",
      "Accuracy val: 0.88\n"
     ]
    }
   ],
   "source": [
    "model = NetDropout(n_chans1=32).to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "all_acc_dict[\"dropout\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetBatchNorm(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv1_batchnorm = nn.BatchNorm2d(num_features=n_chans1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, \n",
    "                               padding=1)\n",
    "        self.conv2_batchnorm = nn.BatchNorm2d(num_features=n_chans1 // 2)\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv1_batchnorm(self.conv1(x))\n",
    "        out = F.max_pool2d(torch.tanh(out), 2)\n",
    "        out = self.conv2_batchnorm(self.conv2(out))\n",
    "        out = F.max_pool2d(torch.tanh(out), 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-16 23:15:35.691938 Epoch 1, Training loss 0.4739942996744897\n",
      "2020-01-16 23:15:40.475810 Epoch 10, Training loss 0.25983829872243724\n",
      "2020-01-16 23:15:45.771541 Epoch 20, Training loss 0.19428231121058676\n",
      "2020-01-16 23:15:51.089952 Epoch 30, Training loss 0.14371838975863852\n",
      "2020-01-16 23:15:56.515419 Epoch 40, Training loss 0.101108748762376\n",
      "2020-01-16 23:16:01.824733 Epoch 50, Training loss 0.06699353904955706\n",
      "2020-01-16 23:16:07.094885 Epoch 60, Training loss 0.041509037291642965\n",
      "2020-01-16 23:16:12.655136 Epoch 70, Training loss 0.032447671671961525\n",
      "2020-01-16 23:16:18.188782 Epoch 80, Training loss 0.017081547878492788\n",
      "2020-01-16 23:16:23.578206 Epoch 90, Training loss 0.011301719506455076\n",
      "2020-01-16 23:16:28.884481 Epoch 100, Training loss 0.007566932796435371\n",
      "Accuracy train: 1.00\n",
      "Accuracy val: 0.89\n"
     ]
    }
   ],
   "source": [
    "model = NetBatchNorm(n_chans1=32).to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "all_acc_dict[\"batch_norm\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetDepth(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3,\n",
    "                               padding=1)\n",
    "        self.conv3 = nn.Conv2d(n_chans1 // 2, n_chans1 // 2,\n",
    "                               kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(4 * 4 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv3(out)), 2)\n",
    "        out = out.view(-1, 4 * 4 * self.n_chans1 // 2)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-16 23:16:30.447670 Epoch 1, Training loss 0.6548013091087341\n",
      "2020-01-16 23:16:35.367838 Epoch 10, Training loss 0.34000502014236084\n",
      "2020-01-16 23:16:40.826647 Epoch 20, Training loss 0.30152006637138923\n",
      "2020-01-16 23:16:46.217950 Epoch 30, Training loss 0.2726998861618103\n",
      "2020-01-16 23:16:51.688735 Epoch 40, Training loss 0.24409755509180628\n",
      "2020-01-16 23:16:57.099919 Epoch 50, Training loss 0.21648093004515218\n",
      "2020-01-16 23:17:02.744809 Epoch 60, Training loss 0.19037676303629664\n",
      "2020-01-16 23:17:08.267520 Epoch 70, Training loss 0.16683378478713856\n",
      "2020-01-16 23:17:13.854005 Epoch 80, Training loss 0.14403212810777555\n",
      "2020-01-16 23:17:19.896823 Epoch 90, Training loss 0.12033685920819355\n",
      "2020-01-16 23:17:25.857992 Epoch 100, Training loss 0.09564469111668077\n",
      "Accuracy train: 0.95\n",
      "Accuracy val: 0.90\n"
     ]
    }
   ],
   "source": [
    "model = NetDepth(n_chans1=32).to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "all_acc_dict[\"depth\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetRes(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3,\n",
    "                               padding=1)\n",
    "        self.conv3 = nn.Conv2d(n_chans1 // 2, n_chans1 // 2,\n",
    "                               kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(4 * 4 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
    "        out1 = out\n",
    "        out = F.max_pool2d(torch.relu(self.conv3(out)) + out1, 2)\n",
    "        out = out.view(-1, 4 * 4 * self.n_chans1 // 2)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-16 23:17:26.950170 Epoch 1, Training loss 0.6650038665267313\n",
      "2020-01-16 23:17:32.211548 Epoch 10, Training loss 0.3292607384122861\n",
      "2020-01-16 23:17:37.607961 Epoch 20, Training loss 0.2860302617595454\n",
      "2020-01-16 23:17:43.125477 Epoch 30, Training loss 0.2551692724227905\n",
      "2020-01-16 23:17:48.706900 Epoch 40, Training loss 0.22809805450545753\n",
      "2020-01-16 23:17:54.233746 Epoch 50, Training loss 0.20181633408661862\n",
      "2020-01-16 23:17:59.702800 Epoch 60, Training loss 0.17625007239781368\n",
      "2020-01-16 23:18:05.151562 Epoch 70, Training loss 0.15140700171802454\n",
      "2020-01-16 23:18:10.695097 Epoch 80, Training loss 0.1257421809491838\n",
      "2020-01-16 23:18:16.346922 Epoch 90, Training loss 0.09920599323454177\n",
      "2020-01-16 23:18:22.144790 Epoch 100, Training loss 0.07639109212786528\n",
      "Accuracy train: 0.97\n",
      "Accuracy val: 0.90\n"
     ]
    }
   ],
   "source": [
    "model = NetRes(n_chans1=32).to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "all_acc_dict[\"res\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, n_chans):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(n_chans, n_chans, kernel_size=3,\n",
    "                              padding=1, bias=False)  # <1>\n",
    "        self.batch_norm = nn.BatchNorm2d(num_features=n_chans)\n",
    "        torch.nn.init.kaiming_normal_(self.conv.weight,\n",
    "                                      nonlinearity='relu')  # <2>\n",
    "        torch.nn.init.constant_(self.batch_norm.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.batch_norm.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.batch_norm(out)\n",
    "        out = torch.relu(out)\n",
    "        return out + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetResDeep(nn.Module):\n",
    "    def __init__(self, n_chans1=32, n_blocks=10):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.resblocks = nn.Sequential(\n",
    "            *(n_blocks * [ResBlock(n_chans=n_chans1)]))\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = self.resblocks(out)\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-16 23:18:30.438073 Epoch 1, Training loss 2.2735002310412704\n",
      "2020-01-16 23:19:39.382842 Epoch 10, Training loss 0.3779076840847161\n",
      "2020-01-16 23:20:55.438525 Epoch 20, Training loss 0.3001826848763569\n",
      "2020-01-16 23:22:12.180387 Epoch 30, Training loss 0.24923191243296217\n",
      "2020-01-16 23:23:29.717063 Epoch 40, Training loss 0.20788565244834134\n",
      "2020-01-16 23:24:45.533130 Epoch 50, Training loss 0.15866709291745143\n",
      "2020-01-16 23:26:01.732320 Epoch 60, Training loss 0.12134665039599321\n",
      "2020-01-16 23:27:17.569136 Epoch 70, Training loss 0.08729177155787018\n",
      "2020-01-16 23:28:33.241105 Epoch 80, Training loss 0.07246267570740288\n",
      "2020-01-16 23:29:49.378612 Epoch 90, Training loss 0.05779321811156003\n",
      "2020-01-16 23:31:05.654037 Epoch 100, Training loss 0.06602069945222917\n",
      "Accuracy train: 0.97\n",
      "Accuracy val: 0.86\n"
     ]
    }
   ],
   "source": [
    "model = NetResDeep(n_chans1=32, n_blocks=100).to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=3e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "all_acc_dict[\"res deep\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAErCAYAAADNILQcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3XmcVnX5//HXWxZZRHDBFQ1MviqampJa2jfNXcv9l5pZkkbmbllafUvbbdGytIjM1MqlQlOTXDJLM01Rcc8kxJwoRVRcEgm9fn9cZ+AwzDBnhHPfN/B+Ph7zYO6zzH1xz5xznc+uiMDMzKw7KzQ7ADMzWzo4YZiZWSVOGGZmVokThpmZVeKEYWZmlThhmJlZJbUlDEkXSHpa0oNd7Jek70qaIul+SVuV9u0h6dFi32l1xWhmZtXVWcK4ENhjEfv3BEYWX2OBHwBI6gWcV+wfBRwqaVSNcZqZWQW1JYyIuAV4dhGH7AtcHOkOYIiktYFtgCkRMTUi5gCXFceamVkTNbMNY13gydLrtmJbV9vNzKyJejfxvdXJtljE9s5/iDSWrNJi4MCBW2+88cZLJjozs+XA3Xff/UxEDK1ybDMTRhuwXun1MGA60LeL7Z2KiPHAeIDRo0fHpEmTlnykZmbLKElPVD22mQnjauA4SZcB2wKzIuJfkmYAIyWNAP4JHAK8v4lxmtVu+GnX9uj4aWfuXVMkZl2rLWFIuhTYEVhdUhtwOtAHICLGAROBvYApwH+AMcW+uZKOA64HegEXRMRDdcVpZmbV1JYwIuLQbvYHcGwX+yaSCcXMzFqER3qbmVklThhmZlaJE4aZmVXihGFmZpU4YZiZWSVOGGZmVokThpmZVeKEYWZmlThhmJlZJU4YZmZWiROGmZlV4oRhZmaVOGGYmVklThhmZlZJMxdQMjNbLiwrC2S5hGFmZpU4YZiZWSVOGGZmVonbMMxsqbestBG0OpcwzMysEicMMzOrxFVSSwkXuc2s2VzCMDOzSpwwzMysEicMMzOrxAnDzMwqccIwM7NKak0YkvaQ9KikKZJO62T/KpKulHS/pDslbVbaN03SA5ImS5pUZ5xmZta92rrVSuoFnAfsCrQBd0m6OiIeLh32GWByROwvaePi+J1L+3eKiGfqitHMqnG3boN6SxjbAFMiYmpEzAEuA/btcMwo4CaAiPgrMFzSmjXGZGZmb1CdCWNd4MnS67ZiW9l9wAEAkrYB3gQMK/YFcIOkuyWNrTFOMzOroM6R3upkW3R4fSZwjqTJwAPAvcDcYt/2ETFd0hrAjZL+GhG3LPQmmUzGAqy//vpLLHgzM1tQnSWMNmC90uthwPTyARHxQkSMiYgtgQ8CQ4HHi33Ti3+fBq4kq7gWEhHjI2J0RIweOnTokv9fmJkZUG/CuAsYKWmEpL7AIcDV5QMkDSn2ARwF3BIRL0gaKGlQccxAYDfgwRpjNTOzbtRWJRURcyUdB1wP9AIuiIiHJB1d7B8HbAJcLOk14GHgyOL0NYErJbXHeElEXFdXrGZm1r1aZ6uNiInAxA7bxpW+vx0Y2cl5U4Et6ozNzMx6xiO9zcysEicMMzOrxAnDzMwqccIwM7NKnDDMzKwSJwwzM6vECcPMzCpxwjAzs0pqHbi3NPF8/2Zmi+YShpmZVeKEYWZmlThhmJlZJU4YZmZWiROGmZlV4oRhZmaVOGGYmVklThhmZlaJE4aZmVXihGFmZpU4YZiZWSVOGGZmVokThpmZVeKEYWZmlThhmJlZJU4YZmZWiROGmZlV4oRhZmaVOGGYmVkltSYMSXtIelTSFEmndbJ/FUlXSrpf0p2SNqt6rpmZNVZtCUNSL+A8YE9gFHCopFEdDvsMMDkiNgc+CJzTg3PNzKyBetf4s7cBpkTEVABJlwH7Ag+XjhkFfA0gIv4qabikNYENKpxrLWT4adf26PhpZ+5dUyRmVpc6E8a6wJOl123Ath2OuQ84APiTpG2ANwHDKp4LgKSxwFiA9ddff4kEbmbLuDMG9/D4WfXEsZSpsw1DnWyLDq/PBFaRNBk4HrgXmFvx3NwYMT4iRkfE6KFDhy5OvGZmtgh1ljDagPVKr4cB08sHRMQLwBgASQIeL74GdHeudcNPUNZM/vtbJtVZwrgLGClphKS+wCHA1eUDJA0p9gEcBdxSJJFuzzUzs8aqrYQREXMlHQdcD/QCLoiIhyQdXewfB2wCXCzpNbJB+8hFnVtXrLbsc6O82eKrs0qKiJgITOywbVzp+9uBkVXPNTOz5qk1YZhZTdxGYE3ghPFG+YI1s+WM55IyM7NKnDDMzKwSJwwzM6vECcPMzCpxwjAzs0qcMMzMrBInDDMzq8TjMMw643E2ZgupVMKQNEHS3pJcIjEzW05VTQA/AN4PPCbpTEkb1xiTmZm1oEoJIyJ+FxGHAVsB04AbJf1Z0hhJfeoM0MzMWkPlKiZJqwFHkOtW3AucQyaQG2uJzMzMWkqlRm9JVwAbAz8F3hsR/yp2XS5pUl3B2TLMjcpmS52qvaTOjYjfd7YjIkYvwXjMzKxFVa2S2kTSkPYXklaRdExNMZmZWQuqmjA+EhHPt7+IiOeAj9QTkpmZtaKqCWMFSWp/IakX0LeekMzMrBVVbcO4HviFpHFAAEcD19UWlZmZtZyqCeNU4KPAxwABNwDn1xWUmdlyrUV7EVZKGBHxOjna+wf1hmNmZq2q6jiMkcDXgFFAv/btEbFBTXGZmVmLqdro/ROydDEX2Am4mBzEZ2Zmy4mqCaN/RNwEKCKeiIgzgHfXF5aZmbWaqo3es4upzR+TdBzwT2CN+sIyM7NWU7WEcRIwADgB2Br4APChuoIyM7PW023CKAbpvS8iXoqItogYExEHRsQdFc7dQ9KjkqZIOq2T/YMlXSPpPkkPSRpT2jdN0gOSJnuCQzOz5uu2SioiXpO0tSRFRFT9wUWiOQ/YFWgD7pJ0dUQ8XDrsWODhiHivpKHAo5J+HhFziv07RcQz1f87ZmZWl6ptGPcCV0n6JfBy+8aIuGIR52wDTImIqQCSLgP2BcoJI4BBxbQjKwHPkj2xzMysxVRNGKsCM1mwZ1QAi0oY6wJPll63Adt2OOZc4GpgOjAIOLgYJNj+82+QFMAPI2J8Z28iaSwwFmD99dev9J8xM7OeqzrSe0z3Ry1EnWzrWKW1OzCZTERvJpd+vTUiXgC2j4jpktYotv81Im7pJLbxwHiA0aNHV64yMzOznqk60vsnLHyzJyI+vIjT2oD1Sq+HkSWJsjHAmUXbyBRJj5Mr+90ZEdOL93ha0pVkFddCCcPMzBqjarfa3wDXFl83ASsDL3Vzzl3ASEkjJPUFDiGrn8r+AewMIGlNYCNgqqSBkgYV2wcCuwEPVozVzMxqULVKakL5taRLgd91c87cYpDf9UAv4IKIeEjS0cX+ccCXgAslPUBWYZ0aEc9I2gC4sliCozdwSUR4OnUzsyaq2ujd0Uig2xbmiJgITOywbVzp++lk6aHjeVOBLd5gbGZmVoOqbRgvsmAbxr/JNTLMzGw5UbVKalDdgZiZWWur1OgtaX9Jg0uvh0jar76wzMys1VTtJXV6RMxbAzAingdOryckMzNrRVUTRmfHvdEGczMzWwpVTRiTJJ0t6c2SNpD0beDuOgMzM7PWUjVhHA/MAS4HfgG8Qs40a2Zmy4mqvaReBhZaz8LMzJYfVXtJ3ShpSOn1KpKury8sMzNrNVWrpFYvekYBEBHP4TW9zcyWK1UTxuuS5k0FImk4ncxea2Zmy66qXWM/C/xJ0h+L1/9LsWiRmZktH6o2el8naTSZJCYDV5E9pczMbDlRdfLBo4ATyUWQJgPbAbez4JKtZma2DKvahnEi8DbgiYjYCXgrMKO2qMzMrOVUTRizI2I2gKQVI+Kv5Op4Zma2nKja6N1WjMP4NXCjpOdYeH1uMzNbhlVt9N6/+PYMSTcDgwEvmWpmthzp8YyzEfHH7o8yM7NlTdU2DDMzW845YZiZWSVOGGZmVokThpmZVeKEYWZmlThhmJlZJU4YZmZWiROGmZlVUmvCkLSHpEclTZG00JrgkgZLukbSfZIekjSm6rlmZtZYtSUMSb2A84A9gVHAoZJGdTjsWODhiNgC2BE4S1LfiueamVkD1VnC2AaYEhFTI2IOcBmwb4djAhgkScBKwLPA3IrnmplZA9WZMNYFniy9biu2lZ0LbELOfPsAcGJEvF7xXDMza6A6E4Y62RYdXu9OruC3DrAlcK6klSuem28ijZU0SdKkGTO8ppOZWV3qTBhtwHql18NYeA2NMcAVkaYAjwMbVzwXgIgYHxGjI2L00KFDl1jwZma2oDoTxl3ASEkjJPUFDgGu7nDMP4CdASStSa7iN7XiuWZm1kA9Xg+jqoiYK+k44HqgF3BBRDwk6ehi/zjgS8CFkh4gq6FOjYhnADo7t65Yzcyse7UlDICImAhM7LBtXOn76cBuVc81M7Pm8UhvMzOrxAnDzMwqccIwM7NKnDDMzKwSJwwzM6vECcPMzCpxwjAzs0qcMMzMrBInDDMzq8QJw8zMKnHCMDOzSpwwzMysEicMMzOrxAnDzMwqccIwM7NKnDDMzKwSJwwzM6vECcPMzCpxwjAzs0qcMMzMrBInDDMzq8QJw8zMKnHCMDOzSpwwzMysEicMMzOrxAnDzMwqccIwM7NKak0YkvaQ9KikKZJO62T/JyVNLr4elPSapFWLfdMkPVDsm1RnnGZm1r3edf1gSb2A84BdgTbgLklXR8TD7cdExDeBbxbHvxc4OSKeLf2YnSLimbpiNDOz6mpLGMA2wJSImAog6TJgX+DhLo4/FLi0xnjMzBby375DaNvqVGYP3gBQ5wc98shivceP9lm7R8c/ol/07A0qxNevXz+GDRtGnz59evazS+pMGOsCT5ZetwHbdnagpAHAHsBxpc0B3CApgB9GxPi6AjWz5VfbVqcyaIPRDB/YG6mLhLHOJov1Hv9te75Hx2+yQhdxdKWb+CKCmTNn0tbWxogRI3r2s0vqbMPo7H8cXRz7XuC2DtVR20fEVsCewLGS/rfTN5HGSpokadKMGTMWL2IzW+7MHrwBqy0qWSwDJLHaaqsxe/bsxfo5dSaMNmC90uthwPQujj2EDtVRETG9+Pdp4EqyimshETE+IkZHxOihQ4cudtBmtrzRMp0s2i2J/2OdCeMuYKSkEZL6kknh6o4HSRoMvAu4qrRtoKRB7d8DuwEP1hirmZl1o7Y2jIiYK+k44HqgF3BBRDwk6ehi/7ji0P2BGyLi5dLpawJXFhmxN3BJRFxXV6xmZu2Gf7ezipCuKke6N+3MvRe5/4VZs/jtr3/JwR86qkc/d6/Dj+eSc7/KkMGD3nBsPVVnozcRMRGY2GHbuA6vLwQu7LBtKrBFnbGZmbWCF1+YxeUX/3ihhPHaa6/Rq1evLs+b+NPv1R3aQmpNGGZmtmjnfO0M2p6Yxvt2fye9e/eh/8CBjFxzJSY/9CgP/2EC+3344zw5/d/MfnUOJx55KGM/cCAAw7fdm0m//RkvvfwKe+78fnbYYQf+/Oc/s+6663LVVVfRv3//JR6rpwYxM2uiEz99BsPeNJxfXH8rJ//fF3lw8j185dRjefgPEwC44KzTufu6S5g08Wd894LLmPnswl10H3vsMY499lgeeughhgwZwoQJE2qJ1SUMM7MWstmWWzFi/XXnvf7uBZdy5W9vBuDJ6U/x2OP/YLVVhyxwzogRI9hyyy0B2HrrrZk2bVotsTlhmJm1kP4DBsz7/g9/nsTvbr2T26+5kAH9+7PjQR9h9qtzFjpnxRVXnPd9r169eOWVV2qJzVVSZmZNNHCllfjPyy91um/Wiy+xyuBBDOjfn79OeZw77nmgwdEtyCUMM7OSaSess/DGdd5a2/sNWWVVthy9LQfs/Hb69evPqqUByHvs+A7G/fRXbL7L+9hog+Fst9VbaoujCicMM7MmO/Pc8ztseRyAFVfsy29/dm6n50z7y7UArL7qKjz44PxxzaecckotMYKrpMzMrCInDDMzq8QJw8zMKnHCMDOzSpwwzMysEicMMzOrxN1qzczKxu+4ZH/eGbOW6I9baeT2vPTYbUv0Z1blEoaZmVXiEoaZWRN9+6uns866681bD+MHZ5/JWis8zy133MNzs17kv3Pn8uVPHcO+u+/Y3EBxCcPMrKn22OdArr/mynmvb/jNrxlz8D5c+eOzuOf6S7j5lz/kE188m4hoYpTJJQwzsybaZLPNeXbmMzz973/x3LPPsPLgway9xuqcfMZZ3PKXe1hBK/DPf8/gqRkzWWuN1ZsaqxOGmVmT7bLXPtw48WpmPv0Uu+9zID+/4rfMmPkcd//25/Tp04fh2+7d6bTmjeYqKTOzJttjnwO4/uoJ3Djxanbdax9mvfgSa6y+Kn369OHm2+7iibZ/NTtEwCUMM7MFjf3DwttqnN4cYMONNuHll15ijbXWZuiaa3HYAXvy3g+dxOg9D2PLTTdi4w2H1/r+VTlhmJm1gAm/+/O871dfdRVuv+aiTo9r1hgMcJWUmZlV5IRhZmaVOGGY2XIuWmKMQ92WxP/RCcPMlmv9Zk1l5stzl+mkERHMnDmTfv36LdbPcaO3mS3Xht3zddo4lRmDNwDU+UGzHlms93jquVd6dPwjmtGzN6gQX79+/Rg2bFjPfm4HThhmtlzrM+d5Rtzx6UUftJgzzu552rU9On5av/f37A2W8Iy4Xam1SkrSHpIelTRF0mmd7P+kpMnF14OSXpO0apVzzcyssWpLGJJ6AecBewKjgEMljSofExHfjIgtI2JL4NPAHyPi2SrnmplZY9VZwtgGmBIRUyNiDnAZsO8ijj8UuPQNnmtmZjVTXT0DJB0E7BERRxWvDwe2jYjjOjl2ANAGbFiUMHpy7lhgbPFyI+DRWv5DC1sdeKZB7/VGOL7F4/gWj+NbPI2M700RMbTKgXU2enfW3aCr7PRe4LaIeLan50bEeGB8z8NbPJImRcToRr9vVY5v8Ti+xeP4Fk+rxldnlVQbsF7p9TBgehfHHsL86qienmtmZg1QZ8K4CxgpaYSkvmRSuLrjQZIGA+8CrurpuWZm1ji1VUlFxFxJxwHXA72ACyLiIUlHF/vHFYfuD9wQES93d25dsb5BDa8G6yHHt3gc3+JxfIunJeOrrdHbzMyWLZ5LyszMKnHCMDOzSpwwzMyWUpK6mC2xHk4YNWn0L3JpJGmFDq9b/jOTtH6zY7DlU/l6kTQQICKi43VUJyeMmkQL9iZo5B9WFRHxutKXJY1o/8xaLXFIequkXpLeDnypk/0tFW+7VoyrVWKStLqkN3fY1hKxdUaSiutlkKTvAuMlXSNpk4h4vVFxtNQNZGlWTJiIpN0lfVbSaZJ2amI8vYt/V5a0nqQ12v+wWuHCKCWv/wccDTxYJI4VWzBxvAl4FvgN8BMASX3ad7biw0FBkNP0SDpa0v6SNm7Ym8+/JjaUtAu01Gf1LeB/YX6cxdN6q/zNLaD0uX0OGAJ8Hbgb+IOkExsVhxPGElBk/9eKQYhnA48AJwLtU7UPanRMxViWXsANwBeBiyWdLKlXsy/a0tPS+sBHge2B3YHtgL9LOhha5+YSEb8GPknONnCZpKMj4r8Akk6RtE5TA+xC8RmvA3wV2BzYFfiQpEMkrVvne0saVFwT6wITgK9LekDSB4vBuE0j6T3ABhHxE0nrAeMk/URS31b5mytrT2Kl6tDTI+L+iDgDOADYtP0BsW5OGEtA6Y/sCOBHwO+AqRExoZhYcWx7nWMjSDqi+PYQMnl9EjgL2Aq4SlIPV2dZskqf1/7AMxHxaET8KSJ2Aa4BzpX0K0krNS/KBS7UAcADEfEWMubjJd0v6VPAIRHRytPWbAOcGxHHkIPBniYT8wkq1p5Z0oqb1/GSdgNOAX4YEVsD/0dOFDqhvcTRJKOAP0raHvgs+SCwErB3E2PqUul6OQY4ufi33b3kg8DiLaVXkRPGkvUY+Yu7CTiz2PZhYJfySPa6FO0BQ4FTJN0JvBu4MiKeAf5AXrxXAe+oO5aKfg28LmnT0rY7yWL3DGCDpkRVKF2oRwIfl7RfRNweEZuSN9/1gI/B/GqNVlCqCnozsDawX1HVNzkivg38Fri7NNnnkrYeMJi8ka0JrAgQEVdFxA5kVcqeNb13l0rVTb8ik+b5wCURcTpZ5bh2o2PqTrmKLCJOI0viOxftF6cBxwMXR8S0RrRReqT3ElQUtc8mn2C+DswmF4L6YETc0+BY3gOcQc7ye0T71CpF3XuviJjdyHg6krRCUWVyHFkt9StgLplgNwd+DlwWEZc1MUxg3hPzMeQN8O/kTebO5kbVPUl3A78H3k5O/f+FiDi3wzGqoxqmuBY+QF4L/0POBXd76e9Q7T18GtVoW8S0BrAO8AAwICJmSnofcEpEbNOIOKoq/24k7QMMImsubpf0OWAM8DhweERMb8Rn6YSxGIr2gNckrUFeFA+SN71PkX+U/YCbIuInDYpn5Yh4QdKOwEoR8RtJXwYOA64AvhoRMxsRSxfxtX9e+5FJYRfgI8BA4H3kzfivZGPt1yPi7c2KtZ2kjSLi0eL7YWS1yvvIEuS329syWkXpRrwj8OGI+GCxfVfyYWZt4G3AE3XcXCRtBEwjf7crA9cB+wGbAa8CU4FfAC82ur1A0rfIv7U3AZMj4jOS+gPvB56OiGsaGU93Sg9Vp5LXyyxgZETsWuxfHfgG8B7gYxExofagIsJfi/lFzq57ZfEL/VSTYuhL1g+fCPyFLFW071uPfIL/O5lImhFf+8NJf/Lp7s1kcvhEh/29gQ8C72yB3+vm5M3tRGBYaftl5AXa9L+9LuLuRVbrTSbbsVYq7fsoWcKs4337k2vb/AT4J7BDad8mwGeAzzfpM3krWd05EJgE7FxsfwfQv9m/s0XEvQrZfgbwS+Do4vv3AOsU3+9ELj5Xezxuw3iDSvXEh5O/0P3JetH9JE2V9JFif6M+415k0joAGAnMkrSmpH4R8SS5BO4OEfFSg+JZQBR/2cyvfuoNzIyIs4rG7e9KWjci5kbExRFxazPi7GAm2c4yDPiUpCOUYzGGAj+Glur6W/5bOxR4gYx9O+AoSdsARMQPI0t5S/zvMiJeIdsn1iWrY98jaf9i3yPAfWRVYzM+t7WAHwDvJKt1bip6NX6erOppVX2A30g6AFg55s/y/TnyoYuIuDkipjQimIZ0xVoWFRfdIGBb4N+S+hQXxTskHUKOLfhRNKB+VtIo4HDyyfcbZD/tMcCBZI+jN5Fd8UbVHUsX8bVXk4jsBbUX8D3yYoV8WtowIv7ZjPjKStUAA8gbyRXksr9bkHGPJXv9zGlk/Xt3is/49aIb6wcjYjdJ/YB9gS2BMZLWjoirILvc1vD+EVmX/kFyidHtgV0lbUl+lutExCHF+zesOkq55PN+ZFXU5uQTOcDpwOMR8XSjYnkDZgAvk6W2rwNI+gTwj2Y8VDlhvAGSdo2IG8kGtCHkE/1Bku4FHotsqG1kY+0qZI+3Q8mqgPOBy8mn+S8ALwFfa2A8CyjdHE4DbicHTG0PzJZ0IPAJ4ASY387RjDhLyWI48EOyp89cskfUpRFxgaTBETELlvxNd3GUPuNdgKeUYwpmA5cXPeb2IZ/wa33/oq1kG7Ja9FZgSvF6c3I8CE1ItNuQA/VeJMckfVXSU2RHgN0bGEePFMm/L1lSnAHsJOlh4H6yO3DDP0s3evdQUbRfh+ymukFE3FMUuw8g+3P/Bbg1ImY0KJ72p/fNyYvhreRgvZvIpDEA6BdNauwu3YQ3B74UEfsW208iP7NbgGkRcX5dPXZ6StIFwENFddm7gU+TSffQaHLvskUpGkHPBjYGfkY2OE+p+4ZS+h3vS06dci3ZM+tB4OyImFo6tqG/46K34K+BMyLiy8pu55uSD3q3R8RTjYqlilLHkIPJWRDmAM+TPR5fIbspPxER/23G9eKE0QOSekfE3OL79wMHAbcBPyXrbI8iL5SPRn193DvG1J4wriLbMB4kq8l2IP/QJgLXNvtpWNJZ5EX62Yj4d7FtgdJEKySMoj3lx8AvotTrRNKlwCcjoq1pwVVQVPu9ixw78jTwR3LMRe3VfZK+B1wRETcru29/iazGe3fkWKCGK7rSHgF8nHwyPyMiHm5GLFUV7UsPADsD3wfuKZLdW8j2l9rHdHXFjd498ylJ35G0Jtl75vtktdSZZD38eWQvhoYkC5g3/83a5EjVX0bEpIg4jxygtx3ZA6RpyUJppSK+TYHDJa1fVJksUPXU7GRRxPAS+dntppwDaZUi/m2BlqmCalfqfLGHpI+TJYvXyJvkY2Rb2vAGxLEnWbrdp2gr+W/kQLN/NuL9uxIRcyJiPPNLPJdKOl8tNNCyE5uSgytXILtBtw8C/iJZtdc0LmFUVGT9d5LdBjci58e5iOxrvhf5NDCLHABU+4daJK1nY/6cRueST/DfjIj7im3XklNXvFh3PFVIegf5pBdkY/LE9vaAZipVA/Qh24NeIC/SdcgGx2HAzRHx1RZs6A5JKwN/IttdRI4D+j05CHKziLi/AbFsSpa4tyJLtc+Rpe7TI6cFaQlFnLtExDnNjqVM0psj4u/F9ysB3ySrbL8eEWcrxy6dFBE7NjFMJ4yeKIr7q5PVPQeQDVI/jIjfS9qQbCt4sEGxfIW8MKdGxL8krUbOM9OPnFJjAFlH+4VGxNNFjO112x2rng4nxzZ8JCLubVZ8RSztN93+ZGeBzcjG4fHAf8mxBc9HxOTy8U0LuBNFr5m3RMQRpW0XAT+OiFtqfN/2z24Q+SQ8FdiarA7bAngYmBARv6krhmVB8TB6C1mCPTki7pb0P2Tb2X/IB5dVgS9GdgduXseQFvvbb0nlp8qiKmWOcpbLPYHdyCfSMyLiHw2Oqz95Ud5H9pp4liz9bEhO6vfrRsZTimuBm2pniUM5PqTpDcilm94XyIvyE+T8UAeQpYozmhlfFZK2Bo6JiCNL274M9ImIU2t6z96RMyJvBXwF+BdZAt+IHDuwF9mVdQbZAeO6Vku0rUbSN8lZGW4kS4kDgPWBEeTD36NNDA9wG0YlpWTxGXIq5N+RVRcXk/WKM8nqoIbQ/EFXO5AxAqDAAAALcklEQVQz484gxzecCjwcEec3MVmsBnxW0q2SPgzzP79Ssmj6XFZFHO3JYihZ7XRtUed9DjkP0juKB4NW9zdgZUmPSDqg6Nn1HuBSqGfwaHvnD7Jh+3tkF9rJxe96BHA9ObL7dTJxOVl0oai5gKyqvYisIZhEJtxbIuLCVkgW4HEY3So9Hb+bLFG8l5wr5/Xipnc/OX14Q+MpXt5DTm3wCJks/g/4m3K9hmZN2ncO0EbeRE6S9LaI+Fi51NGs4nRHpZvYnsBbgNUkPQs8EhFPStqAfBB4slkxdqbU5rIJWUXaOyL+X1HPfQJ5szk/IiYXn/uSHqS3BTnp3WvFvzeTvbGOKg45Brg3cg61Ty3J914WFQ8thwOjI+LdMK/7/lXkNbRLRDzW1CALrpKqSNL3yRk31wZ2jIgPKVfUO4ysDpjT4HhOIPu79wPGkRfukWQvimcj4vFGxlPEtDVwYeS6ESjXW7gQGBM5K2g/8vp4tdGxddRJtdmbyamiVybnG3oGeDkiWvaGpxzE9QBZJdqLnFng9g7HLNE2F+VgstPJHli/Bz5EduO9KiI+XzQq/wrYLiJmtWKbTytSrg9yGPk3OKeo9t6fnK3h8Fb5DF0l1Y1Scb59Xp7jyXpuyFkupzchWWxYxHENOe3DeHLq7UuBvzcjWbSHRpYwUK6/8Cw58Gi7Yv84shdN07VfgJIOlPRZ4F0RcRJZJTCXotOApKGlKoOmK3Wj3ZnsRn0w2Z37dnLOqAtVWk2vhhvNc+S1sAZZovgnOQvtwUU70HfIBZtmqQVWd1yK3EtOn/J5YO2idPth4KKiBNIS3YBdwuhCqX67F7kAzFrkqnXrkL/Ud5I36bc3o5ulpL0oqqDIgWZ7FV9vj4j/NCGeNSPiKeUcTK+SVXahXO9iBNnweUa0wJoDpSqdk8gpMyYAB5PVO+8nqxyPIn/HV0fEj5sVa2ckrULO9Ht5RLRPqTKELF1uFBE/qul92z+3QcAlZGnsSbJadm1yJPIF0aCJ8JY1ktYi54saRn6uvSPiA82NakFOGF0otV18GfhPZB/83uRAqE3JG/XtEXFHg+N6FzCabOz+J/kUcktE3CFpYDRpFKikR4EnyInv/l3a3p8cc/Em4NjIUcBN6xbYIa7byek+Him2HQdsGhHtq+htTa6T0DJtGKUHmU+QDwy3AGOjmOKiKNm9WmdVkKQfke08Z0saTU5hsS3wZ+DMiHihjvddVhUl2HltTZJGkL3OXi+qppp+vbRzlVQn2hsKi/rY/cmeUb2Bk8h5jz4WEd9uQrIQ+VQ3l6zeuZQcwXqppNHNShaFTchZXf+mXKgGmDfl9RxyvMjNxbZW+ON/lRxNW16W88fAZpJGAkTE3a2ULGB+FVNEnEXOK/QM8JBy4CbkZ13bqHnl4MYXySo7ImcWOJXsqTfNyaJ7pWrFtZXd9KO43/Qp7j2PR8Ts9qruFrleACeMTpUutu3JOm3I2V7/F/iWpMOaFVdEXBMR50SuRncuOZfV38gxBE0TEa9HxPHk9BBvlfQvSUcUu48kG0fnXSzNULpQNyli+jfwU0knFfX+J5AdBlqiR8qiKMdBzIkce7EDMFrSBnW3GUTOLPAzYGvl+iBvKT7X9cgEXO4map0oJYBzmN++R+R0KtHKn5+rpDpRKvZvS1anPAOcEzm99SnAihHxleZGOZ9yLY5WWyp0b7LNpz/ZY+ZfTQ5pnuJp/LrIJWy3I+uNnycn6zs7Ih5ppWqArhQdMhQLjqKvf13nvKHtSq4rsQNZqrk1Is5oxPsvzUpV3Z8GRkXE4cr1aj5FVi1f3uQQF8kJoxtFI+4GEfGgpLeR1Ra7levprWtFw/L3G92TrJM4towcl7AFWbV4A9nLqH324bVi/iy6LdkVtKu4tOAI+lFk+0Ij5jMbSFZNDSSn3I5W/exaSVG9fRG5AmBfsjvta2SPxw9ExN+aGN4ieeBeSakXyCHkoiuvkOt1310c8m7gEieL6iLiO9CYJ9+uFNVNx0h6jOzltgk5fcWjkv4eEbPKv9NWuuGVb8DdxaVc++HAiBjTiNiKNrOXyfaL9m0t89m1ouI6mKtcjuBycv6t0yLiNkk30uSq5e64hFEoVUOtQs78eRLZv30yOXL5RuCGVq+msIUVpcQdyTYogKfI9qmVyCksrqdBT+VvhHJhpEPIucJuIXtuzSn2zUsoyilrDosWWxTIFqiKGkBOl7IWOaXQ4IhoK3q97RgR721qoN1wCaNQulkcT04T/Tj5C/0WmTg2IvvnP9KM+OyNKUqN/5F0K3AsmSSeJDsLrETO1zM8Ik5sYpgLKdonVolcKfH75E2mfcLLa4v/T3u7UEg6Bvi9k0VrKpWuzyVLZNuSqyNeU4xr+R+ym/QCVYytxgmDheqG/0iOujwG+F5E3CnpMnLqcieLpUzpwjsbuCkWHDuwHVm6+DU0t9qsE/sAB0q6HegbEfsBSPoQ8D5gb+AbEfGAcv2Ek8nxQdZiSrUXh5Hd4s8if7/3k1O6rAOcEPPHz7RksgB3q223AoCkQ4E3k8liHeBE5Yjqo8h5pGwp1M3YgWeiWLKzhZIFZCnoL+T63COKXmdExEVkl+DHmF/CeB04qNkdC6xzpYfR4eTs1juRvfSmkeuGnEw2erd8G9By34YhaUhEPK+c6fN0si/5y8AoMnm0AX+MiO81MUxbTMp1Gz5HzgB6N7mOyG3A+yLiH63Yu6fohfQWcgK6oeR0IBPK40RarFRkHZTaLvqTM0t/G1g5IoYX+68E/hQRZ7Xi32BHy3XCkDScnJr5++RCJedHxH1FI+NuZL3iV1ptjIP13NIydqBUfTGAnOTy62TPme3IKWHWA26MiJ81MUyroPS7XJuckPFASceTyzm/SE7t886I2L6pgfbAcp0wAJTrXIwBdgfGRcTnS/vuJesWb21WfLZktfrYgdIT6TeAoeUuspI2JhPepIi4q2lBWiWlhPExoH/RfrY2ORnndmTHmruKXlIt29BdttwnDMgJ24DDKVasI3syvAZ8IiL2bmZstvyRtCZZNbpT5DThA4qeXuuR0+m3/I3FkqRh5IJWf4mIfZsdz+JyozcQEa9GxPnkYL0HyHUmxpFLTJo1TPFU+hTZqL01QJEsegPXke1qtpSIiDZyJPdqku5QLoq01HIJoxPKWWq3jYgLmh2LLR+Us5bOkbQZ2dHicPJG8xtyVuKjyUFeY1upCs2qUU7QeCgwlly7+2DgqaXt9+iEYdZkxcCtHchpIiYAJ0bETZLeQXbp3pKcaeCbEfFMKzXSW88oly0+EvjO0tiZxgnDrMmKhvhjgBPJsSHvJBftal9Qp29pKhAni2XE0vi7dMIwaxGSziK7cs8mp9W/DHgPud74Kc2MzQycMMyaqsPkgf0iYrakg8i67mlkw/d3IuKnbruwZnPCMGuS0nT6GwPHAS8AfwcmkhNf7gy85HFA1iqcMMyaTNJNwMXAQcBgckbk35Ej0Vt6USdbvngchlkTSdqZXN/iImBd4MvAisCngbe3H+dkYa3AJQyzJpK0ITlNyQBgTDHOYnPgk8BREfFqUwM0K/F6GGYNJql35DKdu5Oz0K4PCNhB0gfIQV2/K9ZHWOq6XtqyyyUMswaSNDQiZhRjL+4m54x6CtiAXNXxFeC+Yr0Os5bihGHWIMUU678n54l6HHghIs6TNIRcLe8g4OyIeLI43qULaylu9DZrkKLh+iPkWghHksusEhHPR8RtwNrA+0vHO1lYS3HCMGugiJgSEZ8APgysKukuSR+StBG53vMVMK80YtZSXCVl1iTFDKYfILvSrgIcExEXuyrKWpUThlmTFTOYHgz8qOg95UF61pKcMMxaiEsX1sqcMMzMrBI3epuZWSVOGGZmVokThpmZVeKEYWZmlThhmJlZJU4YZmZWyf8HZpDLDkBtFOAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trn_acc = [v['train'] for k, v in all_acc_dict.items()]\n",
    "val_acc = [v['val'] for k, v in all_acc_dict.items()]\n",
    "\n",
    "width =0.3\n",
    "plt.bar(np.arange(len(trn_acc)), trn_acc, width=width, label='train')\n",
    "plt.bar(np.arange(len(val_acc))+ width, val_acc, width=width, label='val')\n",
    "plt.xticks(np.arange(len(val_acc))+ width/2, list(all_acc_dict.keys()),\n",
    "           rotation=60)\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim(0.7, 1)\n",
    "plt.savefig('accuracy_comparison.png', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
